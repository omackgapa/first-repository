# íŒŒì´ì¬ ì›¹ í¬ë¡¤ë§ ë° ë°ì´í„° ë¶„ì„ ê°•ì˜ìë£Œ
## 3~4ì‹œê°„ ì™„ì„± ê³¼ì •

---

## ğŸ“‹ ê°•ì˜ ê°œìš”

**ê°•ì˜ ëª©í‘œ**: íŒŒì´ì¬ì„ í™œìš©í•œ ì›¹ í¬ë¡¤ë§ ê¸°ìˆ  ìŠµë“ ë° ìˆ˜ì§‘í•œ ë°ì´í„°ì˜ ë¶„ì„ ëŠ¥ë ¥ ë°°ì–‘

**í•„ìš” ë„êµ¬**: Python 3.8+, VS Code, Chrome ë¸Œë¼ìš°ì €

---

## ğŸ“š ëª©ì°¨

### 1êµì‹œ (60ë¶„): ì›¹ í¬ë¡¤ë§ ê¸°ì´ˆ ì´ë¡  ë° í™˜ê²½ ì„¤ì •
### 2êµì‹œ (60ë¶„): Beautiful Soupë¥¼ í™œìš©í•œ ì •ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§
### 3êµì‹œ (60ë¶„): Seleniumì„ í™œìš©í•œ ë™ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§
### 4êµì‹œ (60ë¶„): ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”

---

## ğŸ¯ 1êµì‹œ: ì›¹ í¬ë¡¤ë§ ê¸°ì´ˆ ì´ë¡  ë° í™˜ê²½ ì„¤ì • (60ë¶„)

### 1.1 ì›¹ í¬ë¡¤ë§ì´ë€? (15ë¶„)

ì›¹ í¬ë¡¤ë§(Web Crawling)ì€ ì›¹í˜ì´ì§€ì˜ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

**ì£¼ìš” ìš©ë„**:
- ê°€ê²© ë¹„êµ ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘
- ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ë° ë¶„ì„
- ì†Œì…œë¯¸ë””ì–´ íŠ¸ë Œë“œ ë¶„ì„
- ë¶€ë™ì‚° ì •ë³´ ìˆ˜ì§‘
- í•™ìˆ  ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘

**í¬ë¡¤ë§ vs ìŠ¤í¬ë˜í•‘**:
- **í¬ë¡¤ë§**: ì›¹ì‚¬ì´íŠ¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ë§í¬ë¥¼ ë”°ë¼ê°€ë©° ë°ì´í„° ìˆ˜ì§‘
- **ìŠ¤í¬ë˜í•‘**: íŠ¹ì • ì›¹í˜ì´ì§€ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë§Œ ì¶”ì¶œ

### 1.2 ë²•ì  ê³ ë ¤ì‚¬í•­ ë° ìœ¤ë¦¬ì  í¬ë¡¤ë§ (10ë¶„)

**ê¼­ ì§€ì¼œì•¼ í•  ì›ì¹™ë“¤**:

1. **robots.txt í™•ì¸**: `ë„ë©”ì¸/robots.txt`ì—ì„œ í¬ë¡¤ë§ ì •ì±… í™•ì¸
2. **ì´ìš©ì•½ê´€ ì¤€ìˆ˜**: ì‚¬ì´íŠ¸ì˜ Terms of Service í™•ì¸
3. **ìš”ì²­ ê°„ê²© ì¡°ì ˆ**: ì„œë²„ì— ë¶€í•˜ë¥¼ ì£¼ì§€ ì•Šë„ë¡ ì ì ˆí•œ ë”œë ˆì´ ì„¤ì •
4. **ê°œì¸ì •ë³´ ë³´í˜¸**: ê°œì¸ì •ë³´ëŠ” ìˆ˜ì§‘í•˜ì§€ ì•Šê¸°
5. **ì €ì‘ê¶Œ ì¡´ì¤‘**: ìˆ˜ì§‘í•œ ë°ì´í„°ì˜ ìƒì—…ì  ì´ìš© ì‹œ ì£¼ì˜

```python
# robots.txt í™•ì¸ ì˜ˆì œ
import requests

def check_robots_txt(domain):
    robots_url = f"{domain}/robots.txt"
    try:
        response = requests.get(robots_url)
        print(f"=== {domain}ì˜ robots.txt ===")
        print(response.text)
    except Exception as e:
        print(f"robots.txtë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
check_robots_txt("https://example.com")
```

### 1.3 HTML êµ¬ì¡° ì´í•´ (15ë¶„)

ì›¹í˜ì´ì§€ëŠ” HTML, CSS, JavaScriptë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

**HTML ê¸°ë³¸ êµ¬ì¡°**:
```html
<!DOCTYPE html>
<html>
<head>
    <title>í˜ì´ì§€ ì œëª©</title>
</head>
<body>
    <h1 class="title">ë©”ì¸ ì œëª©</h1>
    <div id="content">
        <p class="description">ì„¤ëª… í…ìŠ¤íŠ¸</p>
        <ul class="list">
            <li>í•­ëª© 1</li>
            <li>í•­ëª© 2</li>
        </ul>
    </div>
</body>
</html>
```

**CSS ì„ íƒì ì´í•´**:
- **íƒœê·¸ ì„ íƒì**: `h1`, `p`, `div`
- **í´ë˜ìŠ¤ ì„ íƒì**: `.title`, `.description`
- **ID ì„ íƒì**: `#content`
- **ë³µí•© ì„ íƒì**: `div.content p`, `ul > li`

### 1.4 í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (20ë¶„)

**í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**:

```bash
# ê¸°ë³¸ ì›¹ í¬ë¡¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install requests beautifulsoup4 lxml

# ë™ì  í¬ë¡¤ë§ì„ ìœ„í•œ Selenium
pip install selenium

# ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install pandas numpy matplotlib seaborn

# ì›¹ë“œë¼ì´ë²„ ê´€ë¦¬ (Chrome)
pip install webdriver-manager

# ê¸°íƒ€ ìœ ìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install fake-useragent time
```

**ê¸°ë³¸ ì„¤ì • ì½”ë“œ**:

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import time
import random
from fake_useragent import UserAgent

# User-Agent ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
ua = UserAgent()

# ê¸°ë³¸ í—¤ë” ì„¤ì •
headers = {
    'User-Agent': ua.random,
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
}

# ì„¸ì…˜ ìƒì„± (ì—°ê²° ì¬ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)
session = requests.Session()
session.headers.update(headers)

print("í™˜ê²½ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
```

---

## ğŸ•·ï¸ 2êµì‹œ: Beautiful Soupë¥¼ í™œìš©í•œ ì •ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§ (60ë¶„)

### 2.1 requestsì™€ Beautiful Soup ê¸°ë³¸ ì‚¬ìš©ë²• (20ë¶„)

**ê¸°ë³¸ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°**:

```python
import requests
from bs4 import BeautifulSoup
import time

def basic_crawling_example():
    # 1. ì›¹í˜ì´ì§€ ìš”ì²­
    url = "https://httpbin.org/html"
    response = requests.get(url, headers=headers)
    
    # 2. ì‘ë‹µ ìƒíƒœ í™•ì¸
    if response.status_code == 200:
        print("âœ… í˜ì´ì§€ ìš”ì²­ ì„±ê³µ!")
    else:
        print(f"âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        return
    
    # 3. HTML íŒŒì‹±
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # 4. ì›í•˜ëŠ” ë°ì´í„° ì¶”ì¶œ
    title = soup.find('title').text
    print(f"í˜ì´ì§€ ì œëª©: {title}")
    
    # 5. ëª¨ë“  ë§í¬ ì¶”ì¶œ
    links = soup.find_all('a')
    for link in links:
        href = link.get('href')
        text = link.text.strip()
        print(f"ë§í¬: {text} -> {href}")

# ì‹¤í–‰
basic_crawling_example()
```

**Beautiful Soup ì£¼ìš” ë©”ì„œë“œ**:

```python
def beautifulsoup_methods_demo():
    html = '''
    <div class="container">
        <h1 id="main-title">ë©”ì¸ ì œëª©</h1>
        <p class="description">ì²« ë²ˆì§¸ ì„¤ëª…</p>
        <p class="description">ë‘ ë²ˆì§¸ ì„¤ëª…</p>
        <ul class="item-list">
            <li data-value="1">í•­ëª© 1</li>
            <li data-value="2">í•­ëª© 2</li>
            <li data-value="3">í•­ëª© 3</li>
        </ul>
    </div>
    '''
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # 1. find() - ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì°¾ê¸°
    first_p = soup.find('p', class_='description')
    print(f"ì²« ë²ˆì§¸ p íƒœê·¸: {first_p.text}")
    
    # 2. find_all() - ëª¨ë“  ìš”ì†Œ ì°¾ê¸°
    all_p = soup.find_all('p', class_='description')
    print(f"ëª¨ë“  p íƒœê·¸ ê°œìˆ˜: {len(all_p)}")
    
    # 3. select() - CSS ì„ íƒì ì‚¬ìš©
    items = soup.select('ul.item-list li')
    for item in items:
        value = item.get('data-value')
        text = item.text
        print(f"í•­ëª© {value}: {text}")
    
    # 4. ì†ì„± ê°’ ê°€ì ¸ì˜¤ê¸°
    title_id = soup.find('h1').get('id')
    print(f"ì œëª©ì˜ ID: {title_id}")

beautifulsoup_methods_demo()
```

### 2.2 ì‹¤ìŠµ 1: ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í¬ë¡¤ë§ (25ë¶„)

ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ í—¤ë“œë¼ì¸ì„ ìˆ˜ì§‘í•˜ëŠ” ì‹¤ìŠµì…ë‹ˆë‹¤.

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime

def crawl_news_headlines():
    """
    ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
    (ì˜ˆì‹œ: ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
    """
    
    # ì˜ˆì‹œ URL (ì‹¤ì œë¡œëŠ” í—ˆê°€ëœ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì‚¬ìš©)
    base_url = "https://httpbin.org/html"  # í…ŒìŠ¤íŠ¸ìš© URL
    
    news_data = []
    
    try:
        # í˜ì´ì§€ ìš”ì²­
        response = session.get(base_url)
        response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        # ì˜ˆì‹œ: headlines = soup.find_all('h2', class_='headline')
        
        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
        headlines = ["AI ê¸°ìˆ  ë°œì „ ê°€ì†í™”", "íŒŒì´ì¬ ì¸ê¸° ì§€ì† ìƒìŠ¹", "ë°ì´í„° ë¶„ì„ ì‹œì¥ í™•ëŒ€"]
        
        for i, headline in enumerate(headlines, 1):
            news_item = {
                'title': headline,
                'url': f"https://example.com/news/{i}",
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            news_data.append(news_item)
            
        print(f"âœ… {len(news_data)}ê°œì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return news_data

def save_news_data(news_data):
    """ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    if news_data:
        df = pd.DataFrame(news_data)
        filename = f"news_headlines_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return df
    else:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

# ì‹¤í–‰
news_data = crawl_news_headlines()
df = save_news_data(news_data)
if df is not None:
    print("\nğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(df.head())
```

### 2.3 ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ (15ë¶„)

ì•ˆì •ì ì¸ í¬ë¡¤ë§ì„ ìœ„í•œ ì—ëŸ¬ ì²˜ë¦¬ ë°©ë²•ì…ë‹ˆë‹¤.

```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
import random

class RobustCrawler:
    def __init__(self, max_retries=3, backoff_factor=1):
        self.session = requests.Session()
        
        # ì¬ì‹œë„ ì „ëµ ì„¤ì •
        retry_strategy = Retry(
            total=max_retries,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "OPTIONS"],
            backoff_factor=backoff_factor
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_page(self, url, timeout=10):
        """ì•ˆì „í•œ í˜ì´ì§€ ìš”ì²­"""
        try:
            # ëœë¤ ë”œë ˆì´ (1-3ì´ˆ)
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            
            print(f"âœ… ì„±ê³µ: {url}")
            return response
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ ({url}): {e}")
            return None
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({url}): {e}")
            return None
    
    def parse_with_retry(self, url, parser_func):
        """íŒŒì‹± í•¨ìˆ˜ì™€ í•¨ê»˜ ì¬ì‹œë„"""
        response = self.get_page(url)
        if response:
            try:
                return parser_func(response)
            except Exception as e:
                print(f"âŒ íŒŒì‹± ì˜¤ë¥˜ ({url}): {e}")
                return None
        return None

# ì‚¬ìš© ì˜ˆì‹œ
def example_parser(response):
    soup = BeautifulSoup(response.content, 'html.parser')
    title = soup.find('title')
    return title.text if title else "ì œëª© ì—†ìŒ"

# í¬ë¡¤ëŸ¬ ì‚¬ìš©
crawler = RobustCrawler()
result = crawler.parse_with_retry("https://httpbin.org/html", example_parser)
print(f"íŒŒì‹± ê²°ê³¼: {result}")
```

---

## ğŸ¤– 3êµì‹œ: Seleniumì„ í™œìš©í•œ ë™ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§ (60ë¶„)

### 3.1 Selenium ê¸°ë³¸ ì„¤ì • ë° ì‚¬ìš©ë²• (20ë¶„)

JavaScriptë¡œ ë™ì ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ì½˜í…ì¸ ë¥¼ í¬ë¡¤ë§í•˜ê¸° ìœ„í•´ Seleniumì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time

class SeleniumCrawler:
    def __init__(self, headless=True):
        """
        Selenium í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        headless: Trueë©´ ë¸Œë¼ìš°ì € ì°½ì„ ë³´ì´ì§€ ì•Šê²Œ ì‹¤í–‰
        """
        self.options = Options()
        
        if headless:
            self.options.add_argument('--headless')
        
        # ì„±ëŠ¥ ìµœì í™” ì˜µì…˜
        self.options.add_argument('--no-sandbox')
        self.options.add_argument('--disable-dev-shm-usage')
        self.options.add_argument('--disable-gpu')
        self.options.add_argument('--window-size=1920,1080')
        
        # ë´‡ íƒì§€ ë°©ì§€
        self.options.add_argument('--disable-blink-features=AutomationControlled')
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = None
    
    def start_driver(self):
        """ë“œë¼ì´ë²„ ì‹œì‘"""
        try:
            self.driver = webdriver.Chrome(
                service=webdriver.chrome.service.Service(ChromeDriverManager().install()),
                options=self.options
            )
            
            # ë´‡ íƒì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… Selenium ë“œë¼ì´ë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False
    
    def get_page(self, url, wait_seconds=10):
        """í˜ì´ì§€ ë¡œë“œ ë° ëŒ€ê¸°"""
        if not self.driver:
            print("âŒ ë“œë¼ì´ë²„ê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            self.driver.get(url)
            
            # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
            WebDriverWait(self.driver, wait_seconds).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {url}")
            return True
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def wait_for_element(self, by, value, timeout=10):
        """íŠ¹ì • ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((by, value))
            )
            return element
        except Exception as e:
            print(f"âŒ ìš”ì†Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {e}")
            return None
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("âœ… ë“œë¼ì´ë²„ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê¸°ë³¸ ì‚¬ìš© ì˜ˆì‹œ
def selenium_basic_example():
    crawler = SeleniumCrawler(headless=False)  # ë¸Œë¼ìš°ì € ì°½ ë³´ì´ê²Œ ì„¤ì •
    
    if crawler.start_driver():
        # í…ŒìŠ¤íŠ¸ í˜ì´ì§€ ë°©ë¬¸
        if crawler.get_page("https://httpbin.org/forms/post"):
            
            # í˜ì´ì§€ ì œëª© ê°€ì ¸ì˜¤ê¸°
            title = crawler.driver.title
            print(f"í˜ì´ì§€ ì œëª©: {title}")
            
            # í¼ ìš”ì†Œ ì°¾ê¸°
            try:
                input_field = crawler.driver.find_element(By.NAME, "custname")
                input_field.send_keys("í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì")
                print("âœ… í…ìŠ¤íŠ¸ ì…ë ¥ ì™„ë£Œ")
                
                time.sleep(2)  # ì ì‹œ ëŒ€ê¸°
                
            except Exception as e:
                print(f"âŒ ìš”ì†Œ ì¡°ì‘ ì‹¤íŒ¨: {e}")
        
        crawler.close()

# ì‹¤í–‰
selenium_basic_example()
```

### 3.2 ë™ì  ì½˜í…ì¸  ì²˜ë¦¬ ë° ëŒ€ê¸° ì „ëµ (20ë¶„)

JavaScriptë¡œ ë¡œë“œë˜ëŠ” ì½˜í…ì¸ ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```python
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys

def dynamic_content_handling():
    """ë™ì  ì½˜í…ì¸  ì²˜ë¦¬ ì˜ˆì œ"""
    crawler = SeleniumCrawler(headless=False)
    
    if not crawler.start_driver():
        return
    
    try:
        # ë™ì  ì½˜í…ì¸ ê°€ ìˆëŠ” í…ŒìŠ¤íŠ¸ í˜ì´ì§€
        test_url = "https://the-internet.herokuapp.com/dynamic_loading/1"
        
        if crawler.get_page(test_url):
            
            # 1. ë²„íŠ¼ í´ë¦­í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë“œ
            start_button = crawler.driver.find_element(By.CSS_SELECTOR, "#start button")
            start_button.click()
            print("âœ… ì‹œì‘ ë²„íŠ¼ í´ë¦­")
            
            # 2. ë¡œë”© ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (ëª…ì‹œì  ëŒ€ê¸°)
            hidden_element = crawler.wait_for_element(By.ID, "finish", timeout=10)
            
            if hidden_element:
                result_text = hidden_element.text
                print(f"âœ… ë™ì  ë¡œë“œ ì™„ë£Œ: {result_text}")
            else:
                print("âŒ ë™ì  ì½˜í…ì¸  ë¡œë“œ ì‹¤íŒ¨")
    
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    finally:
        crawler.close()

def advanced_interactions():
    """ê³ ê¸‰ ìƒí˜¸ì‘ìš© ì˜ˆì œ"""
    crawler = SeleniumCrawler(headless=False)
    
    if not crawler.start_driver():
        return
    
    try:
        # ë“œë¡­ë‹¤ìš´ ë° í˜¸ë²„ í…ŒìŠ¤íŠ¸ í˜ì´ì§€
        test_url = "https://the-internet.herokuapp.com/dropdown"
        
        if crawler.get_page(test_url):
            
            # 1. ë“œë¡­ë‹¤ìš´ ì„ íƒ
            dropdown = Select(crawler.driver.find_element(By.ID, "dropdown"))
            dropdown.select_by_visible_text("Option 1")
            print("âœ… ë“œë¡­ë‹¤ìš´ ì„ íƒ ì™„ë£Œ")
            
            time.sleep(2)
            
            # 2. í˜¸ë²„ í…ŒìŠ¤íŠ¸
            hover_url = "https://the-internet.herokuapp.com/hovers"
            crawler.get_page(hover_url)
            
            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì— ë§ˆìš°ìŠ¤ í˜¸ë²„
            first_image = crawler.driver.find_element(By.CSS_SELECTOR, ".figure img")
            actions = ActionChains(crawler.driver)
            actions.move_to_element(first_image).perform()
            
            # í˜¸ë²„ ì‹œ ë‚˜íƒ€ë‚˜ëŠ” í…ìŠ¤íŠ¸ ëŒ€ê¸°
            hover_text = crawler.wait_for_element(By.CSS_SELECTOR, ".figcaption h5")
            if hover_text:
                print(f"âœ… í˜¸ë²„ í…ìŠ¤íŠ¸: {hover_text.text}")
            
            # 3. ìŠ¤í¬ë¡¤ ì²˜ë¦¬
            crawler.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            print("âœ… í˜ì´ì§€ í•˜ë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤")
            
            time.sleep(2)
    
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    finally:
        crawler.close()

# ì‹¤í–‰
dynamic_content_handling()
advanced_interactions()
```

### 3.3 ì‹¤ìŠµ 2: ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€ í¬ë¡¤ë§ (20ë¶„)

ì†Œì…œë¯¸ë””ì–´ë‚˜ ë‰´ìŠ¤ í”¼ë“œì™€ ê°™ì€ ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```python
def infinite_scroll_crawling():
    """ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì œ"""
    crawler = SeleniumCrawler(headless=False)
    
    if not crawler.start_driver():
        return []
    
    collected_data = []
    
    try:
        # ë¬´í•œ ìŠ¤í¬ë¡¤ í…ŒìŠ¤íŠ¸ í˜ì´ì§€
        test_url = "https://the-internet.herokuapp.com/infinite_scroll"
        
        if crawler.get_page(test_url):
            
            last_height = crawler.driver.execute_script("return document.body.scrollHeight")
            scroll_count = 0
            max_scrolls = 5  # ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì œí•œ
            
            while scroll_count < max_scrolls:
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ë°ì´í„° ìˆ˜ì§‘
                elements = crawler.driver.find_elements(By.CSS_SELECTOR, ".jscroll-added")
                current_count = len(elements)
                
                print(f"ìŠ¤í¬ë¡¤ {scroll_count + 1}: {current_count}ê°œ ìš”ì†Œ ë°œê²¬")
                
                # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
                crawler.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                
                # ìƒˆ ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°
                time.sleep(2)
                
                # ìƒˆë¡œìš´ ë†’ì´ í™•ì¸
                new_height = crawler.driver.execute_script("return document.body.scrollHeight")
                
                # ë” ì´ìƒ ìƒˆ ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                if new_height == last_height:
                    print("âœ… ë” ì´ìƒ ë¡œë“œí•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                last_height = new_height
                scroll_count += 1
            
            # ìµœì¢… ë°ì´í„° ìˆ˜ì§‘
            final_elements = crawler.driver.find_elements(By.CSS_SELECTOR, ".jscroll-added")
            
            for i, element in enumerate(final_elements, 1):
                try:
                    text_content = element.text.strip()
                    if text_content:
                        collected_data.append({
                            'index': i,
                            'content': text_content,
                            'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                except Exception as e:
                    print(f"âŒ ìš”ì†Œ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            print(f"âœ… ì´ {len(collected_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    
    except Exception as e:
        print(f"âŒ ë¬´í•œ ìŠ¤í¬ë¡¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    finally:
        crawler.close()
    
    return collected_data

def save_scroll_data(data):
    """ë¬´í•œ ìŠ¤í¬ë¡¤ ë°ì´í„° ì €ì¥"""
    if data:
        df = pd.DataFrame(data)
        filename = f"infinite_scroll_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… ë¬´í•œ ìŠ¤í¬ë¡¤ ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“Š ìˆ˜ì§‘ ë°ì´í„° ìš”ì•½:")
        print(f"- ì´ í•­ëª© ìˆ˜: {len(df)}")
        print(f"- ìˆ˜ì§‘ ì‹œê°„ ë²”ìœ„: {df['collected_at'].min()} ~ {df['collected_at'].max()}")
        
        return df
    else:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

# ì‹¤í–‰
scroll_data = infinite_scroll_crawling()
df_scroll = save_scroll_data(scroll_data)
```

---

## ğŸ“Š 4êµì‹œ: ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” (60ë¶„ + alpha)

### 4.1 ìˆ˜ì§‘ëœ ë°ì´í„° ì „ì²˜ë¦¬ (20ë¶„)

í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ì „ì— ì •ì œí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

```python
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from wordcloud import WordCloud
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows í™˜ê²½)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

def create_sample_data():
    """ë¶„ì„ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    
    categories = ['ê¸°ìˆ ', 'ê²½ì œ', 'ë¬¸í™”', 'ìŠ¤í¬ì¸ ', 'ì •ì¹˜']
    sources = ['ë‰´ìŠ¤A', 'ë‰´ìŠ¤B', 'ë‰´ìŠ¤C', 'ë‰´ìŠ¤D']
    
    sample_data = []
    
    for i in range(100):
        title = f"ë‰´ìŠ¤ ì œëª© {i+1}: {np.random.choice(categories)} ê´€ë ¨ ê¸°ì‚¬"
        
        # ì œëª©ì— íŠ¹ì • í‚¤ì›Œë“œ ì¶”ê°€
        if 'ê¸°ìˆ ' in title:
            title += " AI, ë¹…ë°ì´í„°, í´ë¼ìš°ë“œ"
        elif 'ê²½ì œ' in title:
            title += " ì£¼ì‹, íˆ¬ì, ê¸ˆë¦¬"
        elif 'ë¬¸í™”' in title:
            title += " ì˜ˆìˆ , ìŒì•…, ì˜í™”"
        
        sample_data.append({
            'title': title,
            'category': np.random.choice(categories),
            'source': np.random.choice(sources),
            'views': np.random.randint(100, 10000),
            'comments': np.random.randint(0, 500),
            'date': pd.date_range('2024-01-01', periods=30, freq='D')[i % 30],
            'content_length': np.random.randint(500, 3000)
        })
    
    return pd.DataFrame(sample_data)

def data_preprocessing(df):
    """ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    print("ğŸ“Š ì›ë³¸ ë°ì´í„° ì •ë³´:")
    print(f"- ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
    print(f"- ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    print(f"- ê²°ì¸¡ê°’: {df.isnull().sum().sum()}ê°œ")
    
    # 1. ê²°ì¸¡ê°’ ì²˜ë¦¬
    df_clean = df.copy()
    df_clean = df_clean.dropna()
    
    # 2. ì¤‘ë³µ ì œê±°
    initial_count = len(df_clean)
    df_clean = df_clean.drop_duplicates(subset=['title'])
    removed_count = initial_count - len(df_clean)
    if removed_count > 0:
        print(f"âœ… {removed_count}ê°œ ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°")
    
    # 3. ë‚ ì§œ í˜•ì‹ í†µì¼
    df_clean['date'] = pd.to_datetime(df_clean['date'])
    
    # 4. í…ìŠ¤íŠ¸ ì •ì œ (ì œëª©)
    df_clean['title_clean'] = df_clean['title'].apply(clean_text)
    
    # 5. íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    df_clean['engagement_ratio'] = df_clean['comments'] / (df_clean['views'] + 1)
    df_clean['date_str'] = df_clean['date'].dt.strftime('%Y-%m-%d')
    df_clean['weekday'] = df_clean['date'].dt.day_name()
    
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_clean)}ê°œ ë ˆì½”ë“œ")
    
    return df_clean

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
    if pd.isna(text):
        return ""
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', str(text))
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€ê²½
    text = re.sub(r'\s+', ' ', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def extract_keywords(text_series):
    """í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜"""
    # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    all_text = ' '.join(text_series.astype(str))
    
    # ë‹¨ì–´ ë¶„ë¦¬ (ê°„ë‹¨í•œ ê³µë°± ê¸°ì¤€)
    words = all_text.split()
    
    # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
    stop_words = {'ì˜', 'ê°€', 'ì´', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ê´€ë ¨'}
    words = [word for word in words if len(word) > 1 and word not in stop_words]
    
    # ë¹ˆë„ ê³„ì‚°
    word_freq = Counter(words)
    
    return word_freq.most_common(20)

# ë°ì´í„° ìƒì„± ë° ì „ì²˜ë¦¬ ì‹¤í–‰
sample_df = create_sample_data()
clean_df = data_preprocessing(sample_df)

print("\nğŸ“‹ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
print(clean_df.head())

print("\nğŸ” ê¸°ë³¸ í†µê³„:")
print(clean_df.describe())
```

### 4.2 íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA) (25ë¶„)

ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ íŒ¨í„´ê³¼ íŠ¹ì„±ì„ íŒŒì•…í•˜ëŠ” ë¶„ì„ì…ë‹ˆë‹¤.

```python
def exploratory_data_analysis(df):
    """íƒìƒ‰ì  ë°ì´í„° ë¶„ì„"""
    
    # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.style.use('seaborn-v0_8')
    fig = plt.figure(figsize=(20, 15))
    
    # 1. ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜
    plt.subplot(2, 3, 1)
    category_counts = df['category'].value_counts()
    plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')
    plt.title('ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ë¶„í¬', fontsize=14, fontweight='bold')
    
    # 2. ì‹œê°„ë³„ ê¸°ì‚¬ ë°œí–‰ ì¶”ì´
    plt.subplot(2, 3, 2)
    daily_counts = df.groupby('date_str').size()
    plt.plot(range(len(daily_counts)), daily_counts.values, marker='o')
    plt.title('ì¼ë³„ ê¸°ì‚¬ ë°œí–‰ ìˆ˜', fontsize=14, fontweight='bold')
    plt.xlabel('ì¼ì')
    plt.ylabel('ê¸°ì‚¬ ìˆ˜')
    plt.xticks(rotation=45)
    
    # 3. ì¡°íšŒìˆ˜ ë¶„í¬
    plt.subplot(2, 3, 3)
    plt.hist(df['views'], bins=20, alpha=0.7, color='skyblue')
    plt.title('ì¡°íšŒìˆ˜ ë¶„í¬', fontsize=14, fontweight='bold')
    plt.xlabel('ì¡°íšŒìˆ˜')
    plt.ylabel('ë¹ˆë„')
    
    # 4. ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì¡°íšŒìˆ˜
    plt.subplot(2, 3, 4)
    category_views = df.groupby('category')['views'].mean().sort_values(ascending=False)
    plt.bar(category_views.index, category_views.values, color='lightcoral')
    plt.title('ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì¡°íšŒìˆ˜', fontsize=14, fontweight='bold')
    plt.xlabel('ì¹´í…Œê³ ë¦¬')
    plt.ylabel('í‰ê·  ì¡°íšŒìˆ˜')
    plt.xticks(rotation=45)
    
    # 5. ì¡°íšŒìˆ˜ vs ëŒ“ê¸€ ìˆ˜ ìƒê´€ê´€ê³„
    plt.subplot(2, 3, 5)
    plt.scatter(df['views'], df['comments'], alpha=0.6)
    plt.title('ì¡°íšŒìˆ˜ vs ëŒ“ê¸€ ìˆ˜', fontsize=14, fontweight='bold')
    plt.xlabel('ì¡°íšŒìˆ˜')
    plt.ylabel('ëŒ“ê¸€ ìˆ˜')
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    correlation = df['views'].corr(df['comments'])
    plt.text(0.05, 0.95, f'ìƒê´€ê³„ìˆ˜: {correlation:.3f}', 
             transform=plt.gca().transAxes, fontsize=12, 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    # 6. ìš”ì¼ë³„ ì°¸ì—¬ë„ ë¶„ì„
    plt.subplot(2, 3, 6)
    weekday_engagement = df.groupby('weekday')['engagement_ratio'].mean()
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    weekday_engagement = weekday_engagement.reindex(weekday_order)
    
    plt.bar(range(len(weekday_engagement)), weekday_engagement.values, color='lightgreen')
    plt.title('ìš”ì¼ë³„ í‰ê·  ì°¸ì—¬ë„', fontsize=14, fontweight='bold')
    plt.xlabel('ìš”ì¼')
    plt.ylabel('ì°¸ì—¬ë„ (ëŒ“ê¸€/ì¡°íšŒ)')
    plt.xticks(range(len(weekday_engagement)), 
               ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼'], rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    # ê¸°ë³¸ í†µê³„ ì¶œë ¥
    print("ğŸ“Š ì£¼ìš” í†µê³„ ì •ë³´:")
    print(f"â€¢ í‰ê·  ì¡°íšŒìˆ˜: {df['views'].mean():.0f}")
    print(f"â€¢ í‰ê·  ëŒ“ê¸€ ìˆ˜: {df['comments'].mean():.1f}")
    print(f"â€¢ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì¹´í…Œê³ ë¦¬: {df['category'].mode()[0]}")
    print(f"â€¢ ì¡°íšŒìˆ˜ ìƒìœ„ 10% ê¸°ì¤€: {df['views'].quantile(0.9):.0f}")

def correlation_analysis(df):
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì˜ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
    numeric_cols = ['views', 'comments', 'content_length', 'engagement_ratio']
    correlation_matrix = df[numeric_cols].corr()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, linewidths=0.5)
    plt.title('ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤', fontsize=16, fontweight='bold')
    plt.show()
    
    print("ğŸ” ì£¼ìš” ìƒê´€ê´€ê³„:")
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr = correlation_matrix.iloc[i, j]
            var1 = correlation_matrix.columns[i]
            var2 = correlation_matrix.columns[j]
            print(f"â€¢ {var1} - {var2}: {corr:.3f}")

# ë¶„ì„ ì‹¤í–‰
exploratory_data_analysis(clean_df)
correlation_analysis(clean_df)
```

### 4.3 ë°ì´í„° ì‹œê°í™” ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ (15~40ë¶„)

ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.

```python
def advanced_visualization(df):
    """ê³ ê¸‰ ì‹œê°í™” ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ"""
    
    # 1. í‚¤ì›Œë“œ ë¶„ì„ ë° ì›Œë“œí´ë¼ìš°ë“œ
    keywords = extract_keywords(df['title_clean'])
    
    if keywords:
        print("ğŸ” ìƒìœ„ í‚¤ì›Œë“œ:")
        for word, freq in keywords[:10]:
            print(f"â€¢ {word}: {freq}íšŒ")
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (í•œê¸€ ì§€ì› í°íŠ¸ í•„ìš”)
        try:
            # í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            keyword_dict = {word: freq for word, freq in keywords}
            
            plt.figure(figsize=(12, 6))
            
            # ì›Œë“œí´ë¼ìš°ë“œ
            plt.subplot(1, 2, 1)
            wordcloud = WordCloud(
                width=400, height=300,
                background_color='white',
                max_words=50,
                colormap='viridis',
                font_path='C:/Windows/Fonts/malgun.ttf'  # Windows í•œê¸€ í°íŠ¸
            ).generate_from_frequencies(keyword_dict)
            
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=14, fontweight='bold')
            
        except Exception as e:
            print(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            plt.subplot(1, 2, 1)
            plt.text(0.5, 0.5, 'ì›Œë“œí´ë¼ìš°ë“œ\nìƒì„± ì‹¤íŒ¨', 
                    ha='center', va='center', fontsize=12)
            plt.title('í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=14, fontweight='bold')
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ì°¨íŠ¸
        plt.subplot(1, 2, 2)
        top_keywords = keywords[:10]
        words, freqs = zip(*top_keywords)
        
        plt.barh(range(len(words)), freqs, color='skyblue')
        plt.yticks(range(len(words)), words)
        plt.xlabel('ë¹ˆë„')
        plt.title('ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ', fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        
        plt.tight_layout()
        plt.show()
    
    # 2. ì„±ê³¼ ì§€í‘œ ë¶„ì„
    plt.figure(figsize=(15, 10))
    
    # 2-1. ì†ŒìŠ¤ë³„ ì„±ê³¼ ë¹„êµ
    plt.subplot(2, 2, 1)
    source_performance = df.groupby('source').agg({
        'views': 'mean',
        'comments': 'mean',
        'engagement_ratio': 'mean'
    }).round(2)
    
    x = range(len(source_performance))
    width = 0.25
    
    plt.bar([i - width for i in x], source_performance['views']/100, width, 
            label='í‰ê·  ì¡°íšŒìˆ˜ (Ã—100)', alpha=0.8)
    plt.bar(x, source_performance['comments'], width, 
            label='í‰ê·  ëŒ“ê¸€ ìˆ˜', alpha=0.8)
    plt.bar([i + width for i in x], source_performance['engagement_ratio']*1000, width, 
            label='ì°¸ì—¬ë„ (Ã—1000)', alpha=0.8)
    
    plt.xlabel('ë‰´ìŠ¤ ì†ŒìŠ¤')
    plt.ylabel('ê°’')
    plt.title('ì†ŒìŠ¤ë³„ ì„±ê³¼ ë¹„êµ', fontweight='bold')
    plt.xticks(x, source_performance.index)
    plt.legend()
    
    # 2-2. ì¹´í…Œê³ ë¦¬ë³„ íŠ¸ë Œë“œ
    plt.subplot(2, 2, 2)
    category_trend = df.groupby(['date_str', 'category']).size().unstack(fill_value=0)
    
    for category in category_trend.columns:
        plt.plot(range(len(category_trend)), category_trend[category], 
                marker='o', label=category, linewidth=2)
    
    plt.xlabel('ë‚ ì§œ')
    plt.ylabel('ê¸°ì‚¬ ìˆ˜')
    plt.title('ì¹´í…Œê³ ë¦¬ë³„ ë°œí–‰ íŠ¸ë Œë“œ', fontweight='bold')
    plt.legend()
    plt.xticks(rotation=45)
    
    # 2-3. ì¸ê¸° ê¸°ì‚¬ íŠ¹ì„± ë¶„ì„
    plt.subplot(2, 2, 3)
    top_articles = df.nlargest(20, 'views')
    category_top = top_articles['category'].value_counts()
    
    plt.pie(category_top.values, labels=category_top.index, autopct='%1.1f%%')
    plt.title('ì¸ê¸° ê¸°ì‚¬ ìƒìœ„ 20ê°œ ì¹´í…Œê³ ë¦¬ ë¶„í¬', fontweight='bold')
    
    # 2-4. ì°¸ì—¬ë„ ë¶„í¬
    plt.subplot(2, 2, 4)
    plt.boxplot([df[df['category'] == cat]['engagement_ratio'] for cat in df['category'].unique()],
                labels=df['category'].unique())
    plt.xlabel('ì¹´í…Œê³ ë¦¬')
    plt.ylabel('ì°¸ì—¬ë„')
    plt.title('ì¹´í…Œê³ ë¦¬ë³„ ì°¸ì—¬ë„ ë¶„í¬', fontweight='bold')
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()

def generate_insights(df):
    """ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    insights = []
    
    # 1. ê°€ì¥ ì„±ê³¼ê°€ ì¢‹ì€ ì¹´í…Œê³ ë¦¬
    top_category = df.groupby('category')['views'].mean().idxmax()
    top_category_views = df.groupby('category')['views'].mean().max()
    insights.append(f"'{top_category}' ì¹´í…Œê³ ë¦¬ê°€ í‰ê·  {top_category_views:.0f} ì¡°íšŒìˆ˜ë¡œ ê°€ì¥ ë†’ì€ ì„±ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.")
    
    # 2. ì°¸ì—¬ë„ê°€ ë†’ì€ ì‹œê°„ëŒ€
    best_weekday = df.groupby('weekday')['engagement_ratio'].mean().idxmax()
    insights.append(f"{best_weekday}ì— ë°œí–‰ëœ ê¸°ì‚¬ì˜ ì°¸ì—¬ë„ê°€ ê°€ì¥ ë†’ìŠµë‹ˆë‹¤.")
    
    # 3. ì¡°íšŒìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ì˜ ê´€ê³„
    correlation = df['views'].corr(df['comments'])
    if correlation > 0.5:
        insights.append(f"ì¡°íšŒìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ ê°„ì— ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„({correlation:.3f})ê°€ ìˆìŠµë‹ˆë‹¤.")
    elif correlation > 0.3:
        insights.append(f"ì¡°íšŒìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ ê°„ì— ì¤‘ê°„ ì •ë„ì˜ ì–‘ì˜ ìƒê´€ê´€ê³„({correlation:.3f})ê°€ ìˆìŠµë‹ˆë‹¤.")
    
    # 4. ì½˜í…ì¸  ê¸¸ì´ ìµœì í™”
    optimal_length = df.nlargest(20, 'views')['content_length'].mean()
    insights.append(f"ì¸ê¸° ê¸°ì‚¬ì˜ í‰ê·  ì½˜í…ì¸  ê¸¸ì´ëŠ” {optimal_length:.0f}ìì…ë‹ˆë‹¤.")
    
    # 5. ì†ŒìŠ¤ë³„ ì„±ê³¼ ì°¨ì´
    source_performance = df.groupby('source')['views'].mean()
    best_source = source_performance.idxmax()
    worst_source = source_performance.idxmin()
    performance_ratio = source_performance.max() / source_performance.min()
    insights.append(f"'{best_source}'ê°€ '{worst_source}'ë³´ë‹¤ {performance_ratio:.1f}ë°° ë†’ì€ ì¡°íšŒìˆ˜ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.")
    
    print("ğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    for i, insight in enumerate(insights, 1):
        print(f"{i}. {insight}")
    
    return insights

def create_final_report(df, insights):
    """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
    report = f"""
ğŸ“Š ì›¹ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸
========================================

ğŸ“ˆ ë°ì´í„° ê°œìš”
- ë¶„ì„ ê¸°ê°„: {df['date'].min().strftime('%Y-%m-%d')} ~ {df['date'].max().strftime('%Y-%m-%d')}
- ì´ ê¸°ì‚¬ ìˆ˜: {len(df):,}ê°œ
- ì¹´í…Œê³ ë¦¬ ìˆ˜: {df['category'].nunique()}ê°œ
- ë‰´ìŠ¤ ì†ŒìŠ¤ ìˆ˜: {df['source'].nunique()}ê°œ

ğŸ“Š ì£¼ìš” ì§€í‘œ
- í‰ê·  ì¡°íšŒìˆ˜: {df['views'].mean():.0f}
- í‰ê·  ëŒ“ê¸€ ìˆ˜: {df['comments'].mean():.1f}
- í‰ê·  ì°¸ì—¬ë„: {df['engagement_ratio'].mean():.4f}
- ì´ ì¡°íšŒìˆ˜: {df['views'].sum():,}

ğŸ” ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼
"""
    
    category_stats = df.groupby('category').agg({
        'views': ['count', 'mean', 'sum'],
        'comments': 'mean',
        'engagement_ratio': 'mean'
    }).round(2)
    
    for category in category_stats.index:
        report += f"- {category}: ê¸°ì‚¬ {category_stats.loc[category, ('views', 'count')]}ê°œ, "
        report += f"í‰ê·  ì¡°íšŒìˆ˜ {category_stats.loc[category, ('views', 'mean')]:.0f}\n"
    
    report += f"\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n"
    for i, insight in enumerate(insights, 1):
        report += f"{i}. {insight}\n"
    
    report += f"""
ğŸ“‹ ê¶Œì¥ì‚¬í•­
1. '{df.groupby('category')['views'].mean().idxmax()}' ì¹´í…Œê³ ë¦¬ ì½˜í…ì¸  í™•ëŒ€
2. {df.groupby('weekday')['engagement_ratio'].mean().idxmax()} ë°œí–‰ ìŠ¤ì¼€ì¤„ ìµœì í™”
3. í‰ê·  {df.nlargest(20, 'views')['content_length'].mean():.0f}ì ê¸¸ì´ì˜ ì½˜í…ì¸  ì‘ì„±
4. ìƒìœ„ í‚¤ì›Œë“œ í™œìš©í•œ ì œëª© ìµœì í™”

ë¦¬í¬íŠ¸ ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    with open(f"crawling_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt", 
              'w', encoding='utf-8') as f:
        f.write(report)
    
    print("ğŸ“‹ ë¶„ì„ ë¦¬í¬íŠ¸:")
    print(report)

# ì‹œê°í™” ë° ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤í–‰
advanced_visualization(clean_df)
insights = generate_insights(clean_df)
create_final_report(clean_df, insights)
```

---

## ğŸ¯ ë§ˆë¬´ë¦¬ ë° ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ

### ì¶”ê°€ í•™ìŠµ ìë£Œ
1. **ê³µì‹ ë¬¸ì„œ**
   - [Beautiful Soup ë¬¸ì„œ](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
   - [Selenium ë¬¸ì„œ](https://selenium-python.readthedocs.io/)
   - [Pandas ë¬¸ì„œ](https://pandas.pydata.org/docs/)

2. **ì‹¤ìŠµ í¬ë¡¤ë§ ì‚¬ì´íŠ¸**
   - [Quotes to Scrape](http://quotes.toscrape.com/)
   - [Books to Scrape](http://books.toscrape.com/)
   - [The Internet](https://the-internet.herokuapp.com/)

### í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´
1. **ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°**: ì—¬ëŸ¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ í›„ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„
2. **ë¶€ë™ì‚° ê°€ê²© ëª¨ë‹ˆí„°ë§**: ë¶€ë™ì‚° ì‚¬ì´íŠ¸ì—ì„œ ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘ ë° ê°€ê²© ë³€í™” ì¶”ì 
3. **ì†Œì…œë¯¸ë””ì–´ í•´ì‹œíƒœê·¸ ë¶„ì„**: íŠ¹ì • í•´ì‹œíƒœê·¸ì˜ ì¸ê¸°ë„ ë³€í™” ëª¨ë‹ˆí„°ë§
4. **ì „ììƒê±°ë˜ ê°€ê²© ë¹„êµ**: ì—¬ëŸ¬ ì‡¼í•‘ëª°ì˜ ë™ì¼ ìƒí’ˆ ê°€ê²© ë¹„êµ ì„œë¹„ìŠ¤

### ì£¼ì˜ì‚¬í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] robots.txt í™•ì¸
- [ ] ì´ìš©ì•½ê´€ ê²€í† 
- [ ] ì ì ˆí•œ ë”œë ˆì´ ì„¤ì •
- [ ] User-Agent ì„¤ì •
- [ ] ì—ëŸ¬ ì²˜ë¦¬ êµ¬í˜„
- [ ] ë°ì´í„° ë°±ì—… ê³„íš
- [ ] ê°œì¸ì •ë³´ ì²˜ë¦¬ ë°©ì¹¨ ì¤€ìˆ˜

